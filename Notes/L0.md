# L0: Intro to ML and Fastai

## Usage of DL

- Natural language processing (NLP):: Answering questions; speech recognition; summarizing documents; classifying documents; finding names, dates, etc. in documents; searching for articles mentioning a concept
- Computer vision:: Satellite and drone imagery interpretation (e.g., for disaster resilience); face recognition; image captioning; reading traffic signs; locating pedestrians and vehicles in autonomous vehicles
- Medicine:: Finding anomalies in radiology images, including CT, MRI, and X-ray images; counting features in pathology slides; measuring features in ultrasounds; diagnosing diabetic retinopathy
- Biology:: Folding proteins; classifying proteins; many genomics tasks, such as tumor-normal sequencing and classifying clinically actionable genetic mutations; cell classification; analyzing protein/protein interactions
- Image generation:: Colorizing images; increasing image resolution; removing noise from images; converting images to art in the style of famous artists
- Recommendation systems:: Web search; product recommendations; home page layout
- Playing games:: Chess, Go, most Atari video games, and many real-time strategy games
- Robotics:: Handling objects that are challenging to locate (e.g., transparent, shiny, lacking texture) or hard to pick up
- Other applications:: Financial and logistical forecasting, text to speech, and much more...

## How to learn DL

What you will need to do to succeed however is to apply what you learn in this book to a personal project, and always persevere.
-> Practice directly

The way we will be taught:

- Teaching the *whole game*. We'll start by showing how to use a complete, working, very usable, state-of-the-art deep learning network to solve real-world problems, using simple, expressive tools. And then we'll gradually dig deeper and deeper into understanding how those tools are made, and how the tools that make those tools are made, and so on…
- Always teaching through examples. We'll ensure that there is a context and a purpose that you can understand intuitively, rather than starting with algebraic symbol manipulation.
- Simplifying as much as possible. We've spent years building tools and teaching methods that make previously complex topics very simple.
- Removing barriers. Deep learning has, until now, been a very exclusive game. We're breaking it open, and ensuring that everyone can play.

## What is ML

![ML1](/imgs/ML1.png)
*Machine learning* is, like regular programming, a way to get computers to complete a specific task.

![ML2](/imgs/ML2.png)
Weights are just variables, and a weight assignment is a particular choice of values for those variables. The program's inputs are values that it processes in order to produce its results—for instance, taking image pixels as inputs, and returning the classification "dog" as a result. The program's weight assignments are other values that define how the program will operate.

We've changed the name of our box from *program* to *model*. This is to follow modern terminology and to reflect that the *model* is a special kind of program: it's one that can do *many different things*, depending on the *weights*.


![ML3](/imgs/ML3.png)
Notice the distinction between the model's *results*  (e.g., the moves in a checkers game) and its *performance* (e.g., whether it wins the game, or how quickly it wins). 

Also note that once the model is trained—that is, once we've chosen our final, best, favorite weight assignment—then we can think of the weights as being *part of the model*, since we're not varying them any more.

Therefore, actually *using* a model after it's trained looks like 

![ML4](/imgs/ML4.png)

`Machine Learning: The training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.`

## What is a Neural Network

if you regard a neural network as a mathematical function, it turns out to be a function which is extremely flexible depending on its weights.

The process needed for automatically updating weights is called stochastic gradient descent (SGD).

A neural network is a particular kind of machine learning model. Neural networks are special because they are highly flexible, which means they can solve an unusually wide range of problems just by finding the right weights. 

## DL Jargon
![Training_Detail_Loop](/imgs/ML5.png)

- The functional form of the *model* is called its *architecture* (but be careful—sometimes people use *model* as a synonym of *architecture*, so this can get confusing).
- The *weights* are called *parameters*.
- The *predictions* are calculated from the *independent variable*, which is the *data* not including the *labels*.
- The *results* of the model are called *predictions*.
- The measure of *performance* is called the *loss*.
- The loss depends not only on the predictions, but also the correct *labels* (also known as *targets* or the *dependent variable*); e.g., "dog" or "cat."

## Limitations of ML
From the picture above:
- A model cannot be created without data.
- A model can only learn to operate on the patterns seen in the input data used to train it.
- This learning approach only creates *predictions*, not recommended *actions*.
- It's not enough to just have examples of input data; we need *labels* for that data too (e.g., pictures of dogs and cats aren't enough to train a model; we need a label for each one, saying which ones are dogs, and which are cats).

最要紧的是缺少带标签的数据

## 代码解释

```py
from fastai.vision.all import *
path = untar_data(URLs.PETS)/'images' # download the dataset and return a Path object

def is_cat(x):  # the label is based on the filename
    return x[0].isupper()

dls = ImageDataLoaders.from_name_func( # tell the problem is related to Image
    path,  # way to get the labels from the dataset
    # CV datasets are normally structured in such a way that the label for an image is part of the filename, or path-most commonly the parent folder name
    get_image_files(path), 
    valid_pct=0.2,
    seed=42,
    label_func=is_cat, 
    item_tfms=Resize(224)) # each item isresized to a 224-pixel square. if increased, can get a model with better result, but at the price of speed and memory consumption

learn = vision_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(1)

```

